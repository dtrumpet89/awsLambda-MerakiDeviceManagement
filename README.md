# Meraki AWS Lambda functions

This repo has two lambda functions. One triggers off a S3 bucket
file creation. Which processes the file and drops the parsed
data into a Simple Queue Service. The SQS triggers the second lambda
function to process the queue and compare the new values to the current
Meraki device values. The script will then update only the values that are
different.

## KMS Setup
A new KMS key will need to be created in order to encrypt your Meraki
API key (Strongly Recommended). This key will need to be a symetric key used
for encryption and decryption.

## S3 Setup
Create a folder named merakiConfigCsv in a S3 bucket that will be used for file uploads.
The function will only pickup csv files that are uploaded to the folder.
Also create another folder named merakiConfigCompleted in the same root bucket.
Once the file has been processed, the function will move the file to a the
completed folder and delete the original one out of the upload folder.

## SQS Setup
When setting up a new Simple Queue Service, the queue must be a First In
First Out queue. Otherwise, you may have unexpected behavor due to how the
standard queue is designed.


## Lambda Functions

### S3 Lambda Function
Create a new Lambda function that will trigger from S3.

ARM64 costs less than X86.

#### Code
After creating your function copy and paste the lambda code from the S3 folder.
You will need to create a new file named mLogger.py and then copy the corrisponding
into it.

#### Configuration Trigger
Then select the configuration tab and select Triggers. Add a new S3 trigger.
Event Type: ObjectCreatedByPut (PUT)
Prefix: merakiConfigCsv/
Suffix: .csv

#### Configuration Environment variables
Key: CNF_QUEUE
Value: URL from SQS (https://sqs.us-east-1.amazonaws.com/[UUID]/[SQS Name.fifo])

Key: CNF_TOPIC
Value: ARN: from SQS (arn:aws:sqs:us-east-1:[UUID]:[SQS Name.fifo])

Key: LEVEL
Value: INFO
* Can change to DEBUG for more detailed logging

### SQS Lambda Function
Create a new Lambda function that will trigger from SQS.

ARM64 costs less than X86.

#### Code
After creating your function copy and paste the lambda code from the SQS folder.
Create a new file named mLogger.py and then copy the corisponding code into it.

#### Configuration Trigger
SQS with batch size of 10.

#### Configuration Environment variables
Key: MERAKI_API_KEY
Value: Encrypt this value with the KMS key created earlier

Key: LEVEL
Value: INFO
* Can be changed to DEBUG for detailed logging

#### Layer
The requests module is not avaliable by default. The module has to be added as a layer.
Select the lambda tree at the top and then select Layers under additional resources.
Create a new Layer and upload the requests.zip file in the repo.

Now open the SQS Lambda function and under configuration select Layers and then select the
newly created requests layer.



## Policies
Two new policies will need to be created to allow your Lambda functions to
Read and write from S3 and to publish to SQS.

After you have created the two lambda functions the following policies will need
to be added to their autogenerated roles

### S3 policy
The S3 policy needs to have permission following permissions on the entire bucket.
GetObject
DeleteObject
PutObject

These permissions can be limited down to just the specific folders created in the S3
setup.

### SQS policy
The SQS policy needs to have full read and write permissions to the new SQS


## Sample CSV File
There is a sample CSV file located in the [docs/](docs/) directory.
This tool only requires that the deviceSerial field be present.
All other fields are optional and field order is not specific.

The deviceTags field can be a comma separated list of tags and the function will
break them into individual tags.
